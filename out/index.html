<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/028c0d39d2e8f589-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/5b01f339abf2f1a5.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" as="image" href="/profile.jpg"/><link rel="stylesheet" href="/_next/static/css/2a7316fc55aa7fb8.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-62cf2d8cf4b76051.js"/><script src="/_next/static/chunks/8bc8d761-303d26252bce4712.js" async=""></script><script src="/_next/static/chunks/240-2a4fe7eeef648b82.js" async=""></script><script src="/_next/static/chunks/main-app-483e4d5a160546d2.js" async=""></script><script src="/_next/static/chunks/94-81718c1dcd5eafa0.js" async=""></script><script src="/_next/static/chunks/app/page-d10ce261fb8f3645.js" async=""></script><script src="/_next/static/chunks/app/layout-c96271b5eedd569a.js" async=""></script><title>v0 App</title><meta name="description" content="Created with v0"/><meta name="generator" content="v0.app"/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="font-sans __variable_fb8f2c __variable_f910ec"><div class="min-h-screen bg-background"><header class="border-b border-border bg-card/50 backdrop-blur-sm sticky top-0 z-10"><div class="max-w-7xl mx-auto px-6 py-4"><div class="flex items-center justify-between"><div><h1 class="text-2xl font-bold text-foreground text-balance">Amirhesam Abedsoltan</h1><p class="text-sm text-muted-foreground">PhD Candidate at UC San Diego</p></div><nav class="hidden md:flex space-x-4"><a href="#bio" class="text-sm text-muted-foreground hover:text-primary transition-colors">About</a><a href="#news" class="text-sm text-muted-foreground hover:text-primary transition-colors">News</a><a href="#publications" class="text-sm text-muted-foreground hover:text-primary transition-colors">Publications</a><a href="#hobbies" class="text-sm text-muted-foreground hover:text-primary transition-colors">Hobbies</a></nav></div></div></header><main class="max-w-7xl mx-auto px-6 py-8 space-y-10"><section id="bio" class="space-y-6"><div class="text-center space-y-2"><h2 class="text-2xl font-bold text-foreground text-balance">About Me</h2><div class="w-16 h-0.5 bg-primary mx-auto rounded-full"></div></div><div class="grid md:grid-cols-4 gap-8 items-start"><div class="flex justify-center"><div class="w-48 h-48 bg-muted rounded-full flex items-center justify-center overflow-hidden shadow-lg"><img src="/profile.jpg" alt="Profile photo" class="w-full h-full object-cover"/></div></div><div class="md:col-span-3 space-y-4"><div class="prose max-w-none"><p class="text-base text-foreground leading-relaxed text-pretty">Hi, I&#x27;m Amirhesam Abedsoltan, a Ph.D. candidate at UC San Diego. I was lucky to be advised by<!-- --> <a href="https://misha.belkin-wang.org/" target="_blank" rel="noopener noreferrer" class="text-primary hover:underline">Prof. Mikhail Belkin</a>. My work broadly focuses on machine learning, spanning both theory and practice, with an interest in scaling methods and understanding how modern AI systems learn and generalize.</p></div><div class="space-y-3"><h3 class="text-lg font-semibold text-foreground">Education</h3><div class="space-y-2 text-sm"><div class="flex items-center space-x-2"><div class="w-1.5 h-1.5 bg-primary rounded-full flex-shrink-0"></div><div><span class="font-medium text-foreground">Ph.D. Computer Science</span><span class="text-muted-foreground ml-2">University of California San Diego, 2021 - Present</span></div></div><div class="flex items-center space-x-2"><div class="w-1.5 h-1.5 bg-primary rounded-full flex-shrink-0"></div><div><span class="font-medium text-foreground">M.S. Computer Science</span><span class="text-muted-foreground ml-2">University of Southern California, 2021</span></div></div><div class="flex items-center space-x-2"><div class="w-1.5 h-1.5 bg-primary rounded-full flex-shrink-0"></div><div><span class="font-medium text-foreground">B.S. Electrical Engineering</span><span class="text-muted-foreground ml-2">Sharif University of Technology, 2014</span></div></div></div></div></div></div></section><div data-orientation="horizontal" role="none" data-slot="separator" class="bg-border shrink-0 data-[orientation=horizontal]:h-px data-[orientation=horizontal]:w-full data-[orientation=vertical]:h-full data-[orientation=vertical]:w-px my-8"></div><section id="news" class="space-y-6"><div class="text-center space-y-2"><h2 class="text-2xl font-bold text-foreground text-balance">Recent News</h2><div class="w-16 h-0.5 bg-primary mx-auto rounded-full"></div></div><div class="space-y-3"><div data-slot="card" class="bg-card text-card-foreground flex flex-col gap-6 rounded-xl border py-6 shadow-sm border-border hover:shadow-md transition-shadow"><div data-slot="card-content" class="p-4"><div class="flex items-start space-x-3"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-calendar w-4 h-4 text-primary mt-0.5 flex-shrink-0"><path d="M8 2v4"></path><path d="M16 2v4"></path><rect width="18" height="18" x="3" y="4" rx="2"></rect><path d="M3 10h18"></path></svg><div class="flex-1 space-y-1"><div class="flex items-start justify-between gap-4"><h3 class="text-base font-semibold text-foreground text-balance leading-tight">Our paper, “Fast Training of Large Kernel Models with Delayed Projections.” was spotlighted at NeurIPS.</h3><span data-slot="badge" class="inline-flex items-center justify-center rounded-md border px-2 py-0.5 font-medium w-fit shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden border-transparent bg-secondary text-secondary-foreground [a&amp;]:hover:bg-secondary/90 text-xs whitespace-nowrap">September 2025</span></div><p class="text-sm text-muted-foreground leading-relaxed text-pretty">In this paper, we significantly reduced the runtime of kernel solvers from days to minutes by using delayed projections in Preconditioned Stochastic Gradient Descent.</p></div></div></div></div><div data-slot="card" class="bg-card text-card-foreground flex flex-col gap-6 rounded-xl border py-6 shadow-sm border-border hover:shadow-md transition-shadow"><div data-slot="card-content" class="p-4"><div class="flex items-start space-x-3"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-calendar w-4 h-4 text-primary mt-0.5 flex-shrink-0"><path d="M8 2v4"></path><path d="M16 2v4"></path><rect width="18" height="18" x="3" y="4" rx="2"></rect><path d="M3 10h18"></path></svg><div class="flex-1 space-y-1"><div class="flex items-start justify-between gap-4"><h3 class="text-base font-semibold text-foreground text-balance leading-tight">I will be an AI Research Intern at Figma.</h3><span data-slot="badge" class="inline-flex items-center justify-center rounded-md border px-2 py-0.5 font-medium w-fit shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden border-transparent bg-secondary text-secondary-foreground [a&amp;]:hover:bg-secondary/90 text-xs whitespace-nowrap">June 2025</span></div><p class="text-sm text-muted-foreground leading-relaxed text-pretty">I will be working on multi-agent workflows for prompt-to-design applications extending agent-squad from AWS.</p></div></div></div></div><div data-slot="card" class="bg-card text-card-foreground flex flex-col gap-6 rounded-xl border py-6 shadow-sm border-border hover:shadow-md transition-shadow"><div data-slot="card-content" class="p-4"><div class="flex items-start space-x-3"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-calendar w-4 h-4 text-primary mt-0.5 flex-shrink-0"><path d="M8 2v4"></path><path d="M16 2v4"></path><rect width="18" height="18" x="3" y="4" rx="2"></rect><path d="M3 10h18"></path></svg><div class="flex-1 space-y-1"><div class="flex items-start justify-between gap-4"><h3 class="text-base font-semibold text-foreground text-balance leading-tight">Our paper, “Task Generalization With AutoRegressive Compositional Structure: Can Learning From D Tasks Generalize to Dᵀ Tasks?” was accepted at ICML.</h3><span data-slot="badge" class="inline-flex items-center justify-center rounded-md border px-2 py-0.5 font-medium w-fit shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden border-transparent bg-secondary text-secondary-foreground [a&amp;]:hover:bg-secondary/90 text-xs whitespace-nowrap">August 2025</span></div><p class="text-sm text-muted-foreground leading-relaxed text-pretty">We study task generalization through an autoregressive compositional framework, showing that training on only a small subset of tasks enables exponential generalization to a much larger class.     As an example, we demonstrate that Transformers trained on sparse parity tasks generalize in-context through chain-of-thought reasoning, and further extend to arithmetic and translation.</p></div></div></div></div></div></section><div data-orientation="horizontal" role="none" data-slot="separator" class="bg-border shrink-0 data-[orientation=horizontal]:h-px data-[orientation=horizontal]:w-full data-[orientation=vertical]:h-full data-[orientation=vertical]:w-px my-8"></div><section id="publications" class="space-y-6"><div class="text-center space-y-2"><h2 class="text-2xl font-bold text-foreground text-balance">Selected Publications</h2><div class="w-16 h-0.5 bg-primary mx-auto rounded-full"></div></div><div class="flex flex-wrap gap-3 items-center justify-between bg-card p-3 rounded-lg border border-border"><div class="flex items-center space-x-3"><div class="flex items-center space-x-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-filter w-3 h-3 text-muted-foreground"><polygon points="22 3 2 3 10 12.46 10 19 14 21 14 12.46 22 3"></polygon></svg><select class="bg-background border border-border rounded px-2 py-1 text-xs"><option value="all" selected="">All Topics</option><option value="In-context learning">In-context learning</option><option value="Task generalization">Task generalization</option><option value="Chain-Of-Thought">Chain-Of-Thought</option><option value="Kernel methods">Kernel methods</option><option value="Optimization">Optimization</option><option value="Preconditioning">Preconditioning</option><option value="Transformers">Transformers</option><option value="Nyström approximation">Nyström approximation</option><option value="Uncertainty estimation">Uncertainty estimation</option><option value="Feature Learning">Feature Learning</option><option value="Neural networks">Neural networks</option><option value="Early stopping">Early stopping</option><option value="Overfitting">Overfitting</option><option value="Generalization">Generalization</option><option value="Automatic relevance">Automatic relevance</option></select></div></div><div class="flex items-center space-x-2"><span class="text-xs text-muted-foreground">Sort:</span><button data-slot="button" class="inline-flex items-center justify-center whitespace-nowrap font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg:not([class*=&#x27;size-&#x27;])]:size-4 shrink-0 [&amp;_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive bg-primary text-primary-foreground shadow-xs hover:bg-primary/90 rounded-md gap-1.5 px-3 has-[&gt;svg]:px-2.5 text-xs h-7">Date</button><button data-slot="button" class="inline-flex items-center justify-center whitespace-nowrap font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg:not([class*=&#x27;size-&#x27;])]:size-4 shrink-0 [&amp;_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50 rounded-md gap-1.5 px-3 has-[&gt;svg]:px-2.5 text-xs h-7">Topic</button><button data-slot="button" class="inline-flex items-center justify-center whitespace-nowrap font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg:not([class*=&#x27;size-&#x27;])]:size-4 shrink-0 [&amp;_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50 rounded-md gap-1.5 has-[&gt;svg]:px-2.5 text-xs h-7 px-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-down-wide-narrow w-3 h-3"><path d="m3 16 4 4 4-4"></path><path d="M7 20V4"></path><path d="M11 4h10"></path><path d="M11 8h7"></path><path d="M11 12h4"></path></svg></button></div></div><div class="space-y-4"><div data-slot="card" class="bg-card text-card-foreground flex flex-col gap-6 rounded-xl border py-6 shadow-sm border-border hover:shadow-md transition-shadow"><div data-slot="card-content" class="p-4"><div class="space-y-3"><div class="flex items-start justify-between gap-4"><div class="flex-1"><h3 class="text-base font-semibold text-foreground text-balance leading-tight">Task Generalization With AutoRegressive Compositional Structure: Can Learning From D Tasks Generalize to Dᵀ Tasks?</h3><p class="text-xs text-muted-foreground mt-1">A Abedsoltan, H Zhang, K Wan, H Lin, J Zhang, M Belkin</p></div><div class="flex flex-col items-end space-y-1"><div class="flex flex-wrap gap-1 justify-end"><span data-slot="badge" class="inline-flex items-center justify-center rounded-md border px-2 py-0.5 font-medium w-fit whitespace-nowrap shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden text-foreground [a&amp;]:hover:bg-accent [a&amp;]:hover:text-accent-foreground text-xs">In-context learning</span><span data-slot="badge" class="inline-flex items-center justify-center rounded-md border px-2 py-0.5 font-medium w-fit whitespace-nowrap shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden text-foreground [a&amp;]:hover:bg-accent [a&amp;]:hover:text-accent-foreground text-xs">Task generalization</span><span data-slot="badge" class="inline-flex items-center justify-center rounded-md border px-2 py-0.5 font-medium w-fit whitespace-nowrap shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden text-foreground [a&amp;]:hover:bg-accent [a&amp;]:hover:text-accent-foreground text-xs">Chain-Of-Thought</span></div><span class="text-xs text-muted-foreground">2025</span></div></div><p class="text-xs font-medium text-primary">42nd International Conference on Machine Learning (ICML2025)</p><p class="text-sm text-muted-foreground leading-relaxed text-pretty">Large language models (LLMs) exhibit remarkable task generalization, solving tasks they were never explicitly trained on with only a few demonstrations.     This raises a fundamental question: When can learning from a small set of tasks generalize to a large task family? In this paper, we investigate task generalization     through the lens of autoregressive compositional structure, where each task is a composition of T operations, and each operation is among a finite family of d subtasks.      This yields a total class of size d^T. We first show that generalization to all d^T tasks is theoretically achievable by training on only Õ(d) tasks.       Empirically, we demonstrate that Transformers achieve such exponential task generalization on sparse parity functions via In-context Learning (ICL) and chain-of-thought (CoT) reasoning.        We further show generalization in arithmetic and translation, beyond parity functions.</p></div></div></div><div data-slot="card" class="bg-card text-card-foreground flex flex-col gap-6 rounded-xl border py-6 shadow-sm border-border hover:shadow-md transition-shadow"><div data-slot="card-content" class="p-4"><div class="space-y-3"><div class="flex items-start justify-between gap-4"><div class="flex-1"><h3 class="text-base font-semibold text-foreground text-balance leading-tight">Fast training of large kernel models with delayed projections</h3><p class="text-xs text-muted-foreground mt-1">A Abedsoltan, S Ma, P Pandit, M Belkin</p></div><div class="flex flex-col items-end space-y-1"><div class="flex flex-wrap gap-1 justify-end"><span data-slot="badge" class="inline-flex items-center justify-center rounded-md border px-2 py-0.5 font-medium w-fit whitespace-nowrap shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden text-foreground [a&amp;]:hover:bg-accent [a&amp;]:hover:text-accent-foreground text-xs">Kernel methods</span><span data-slot="badge" class="inline-flex items-center justify-center rounded-md border px-2 py-0.5 font-medium w-fit whitespace-nowrap shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden text-foreground [a&amp;]:hover:bg-accent [a&amp;]:hover:text-accent-foreground text-xs">Optimization</span><span data-slot="badge" class="inline-flex items-center justify-center rounded-md border px-2 py-0.5 font-medium w-fit whitespace-nowrap shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden text-foreground [a&amp;]:hover:bg-accent [a&amp;]:hover:text-accent-foreground text-xs">Preconditioning</span></div><span class="text-xs text-muted-foreground">2025</span></div></div><p class="text-xs font-medium text-primary">39th Conference on Neural Information Processing Systems (NeurIPS 2025-Spotlight)</p><p class="text-sm text-muted-foreground leading-relaxed text-pretty">Classical kernel machines have historically faced significant challenges in scaling to large datasets and model sizes--a key ingredient that has driven the success of neural networks.      In this paper, we present a new methodology for building kernel machines that can scale efficiently with both data size and model size.      Our algorithm introduces delayed projections to Preconditioned Stochastic Gradient Descent (PSGD) allowing the training of much larger models than was previously feasible,       pushing the practical limits of kernel-based learning. We validate our algorithm, EigenPro4, across multiple datasets, demonstrating drastic training speed up over       the existing methods while maintaining comparable or better classification accuracy.</p></div></div></div><div data-slot="card" class="bg-card text-card-foreground flex flex-col gap-6 rounded-xl border py-6 shadow-sm border-border hover:shadow-md transition-shadow"><div data-slot="card-content" class="p-4"><div class="space-y-3"><div class="flex items-start justify-between gap-4"><div class="flex-1"><h3 class="text-base font-semibold text-foreground text-balance leading-tight">Context-Scaling versus Task-Scaling in In-Context Learning</h3><p class="text-xs text-muted-foreground mt-1">A Abedsoltan, A Radhakrishnan, J Wu, M Belkin</p></div><div class="flex flex-col items-end space-y-1"><div class="flex flex-wrap gap-1 justify-end"><span data-slot="badge" class="inline-flex items-center justify-center rounded-md border px-2 py-0.5 font-medium w-fit whitespace-nowrap shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden text-foreground [a&amp;]:hover:bg-accent [a&amp;]:hover:text-accent-foreground text-xs">In-context learning</span><span data-slot="badge" class="inline-flex items-center justify-center rounded-md border px-2 py-0.5 font-medium w-fit whitespace-nowrap shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden text-foreground [a&amp;]:hover:bg-accent [a&amp;]:hover:text-accent-foreground text-xs">Transformers</span></div><span class="text-xs text-muted-foreground">2024</span></div></div><p class="text-xs font-medium text-primary">arXiv preprint arXiv:2410.12783</p><p class="text-sm text-muted-foreground leading-relaxed text-pretty">Transformers exhibit In-Context Learning (ICL), where these models solve new tasks by using examples in the prompt without additional training.     In our work, we identify and analyze two key components of ICL: (1) context-scaling, where model performance improves as the number of in-context examples increases and      (2) task-scaling, where model performance improves as the number of pre-training tasks increases. While transformers are capable of both context-scaling and task-scaling,       we empirically show that standard Multi-Layer Perceptrons (MLPs) with vectorized input are only capable of task-scaling. To understand how transformers are capable of context-scaling,        we first propose a significantly simplified transformer architecture without key, query, value weights. We show that it performs ICL comparably to the original GPT-2 model        in various statistical learning tasks including linear regression, teacher-student settings. Furthermore, a single block of our simplified transformer can be viewed as data dependent         feature map followed by an MLP. This feature map on its own is a powerful predictor that is capable of context-scaling but is not capable of task-scaling. We show empirically that concatenating          the output of this feature map with vectorized data as an input to MLPs enables both context-scaling and task-scaling. This finding provides a simple setting to study context and task-scaling for ICL.</p></div></div></div><div data-slot="card" class="bg-card text-card-foreground flex flex-col gap-6 rounded-xl border py-6 shadow-sm border-border hover:shadow-md transition-shadow"><div data-slot="card-content" class="p-4"><div class="space-y-3"><div class="flex items-start justify-between gap-4"><div class="flex-1"><h3 class="text-base font-semibold text-foreground text-balance leading-tight">Uncertainty estimation with recursive feature machines</h3><p class="text-xs text-muted-foreground mt-1">D Gedon, A Abedsoltan, TB Schön, M Belkin</p></div><div class="flex flex-col items-end space-y-1"><div class="flex flex-wrap gap-1 justify-end"><span data-slot="badge" class="inline-flex items-center justify-center rounded-md border px-2 py-0.5 font-medium w-fit whitespace-nowrap shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden text-foreground [a&amp;]:hover:bg-accent [a&amp;]:hover:text-accent-foreground text-xs">Uncertainty estimation</span><span data-slot="badge" class="inline-flex items-center justify-center rounded-md border px-2 py-0.5 font-medium w-fit whitespace-nowrap shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden text-foreground [a&amp;]:hover:bg-accent [a&amp;]:hover:text-accent-foreground text-xs">Feature Learning</span><span data-slot="badge" class="inline-flex items-center justify-center rounded-md border px-2 py-0.5 font-medium w-fit whitespace-nowrap shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden text-foreground [a&amp;]:hover:bg-accent [a&amp;]:hover:text-accent-foreground text-xs">Kernel methods</span></div><span class="text-xs text-muted-foreground">2024</span></div></div><p class="text-xs font-medium text-primary">The 40th Conference on Uncertainty in Artificial Intelligence</p><p class="text-sm text-muted-foreground leading-relaxed text-pretty">In conventional regression analysis, predictions are typically represented as point estimates derived from covariates.      The Gaussian Process (GP) offer a kernel-based framework that predicts and additionally quantifies associated uncertainties.       However, kernel-based methods often underperform ensemble-based decision tree approaches in regression tasks involving tabular and categorical data.        Recently, Recursive Feature Machines (RFMs) were proposed as a novel feature-learning kernel which strengthens the capabilities of kernel machines.        In this study, we harness the power RFMs in a probabilistic GP-based approach to enhance uncertainty estimation through feature extraction within kernel methods.         We employ this learned kernel for in-depth uncertainty analysis. On tabular datasets, our RFM-based method surpasses other leading uncertainty estimation techniques,         including NGBoost and CatBoost-ensemble. Additionally, when assessing out-of-distribution performance, we found that boosting-based methods are surpassed by our RFM-based approach.</p></div></div></div><div data-slot="card" class="bg-card text-card-foreground flex flex-col gap-6 rounded-xl border py-6 shadow-sm border-border hover:shadow-md transition-shadow"><div data-slot="card-content" class="p-4"><div class="space-y-3"><div class="flex items-start justify-between gap-4"><div class="flex-1"><h3 class="text-base font-semibold text-foreground text-balance leading-tight">On the Nyström approximation for preconditioning in kernel machines</h3><p class="text-xs text-muted-foreground mt-1">A Abedsoltan, P Pandit, L Rademacher, M Belkin</p></div><div class="flex flex-col items-end space-y-1"><div class="flex flex-wrap gap-1 justify-end"><span data-slot="badge" class="inline-flex items-center justify-center rounded-md border px-2 py-0.5 font-medium w-fit whitespace-nowrap shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden text-foreground [a&amp;]:hover:bg-accent [a&amp;]:hover:text-accent-foreground text-xs">Kernel methods</span><span data-slot="badge" class="inline-flex items-center justify-center rounded-md border px-2 py-0.5 font-medium w-fit whitespace-nowrap shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden text-foreground [a&amp;]:hover:bg-accent [a&amp;]:hover:text-accent-foreground text-xs">Nyström approximation</span><span data-slot="badge" class="inline-flex items-center justify-center rounded-md border px-2 py-0.5 font-medium w-fit whitespace-nowrap shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden text-foreground [a&amp;]:hover:bg-accent [a&amp;]:hover:text-accent-foreground text-xs">Preconditioning</span></div><span class="text-xs text-muted-foreground">2024</span></div></div><p class="text-xs font-medium text-primary">International Conference on Artificial Intelligence and Statistics (AISTATS 2024)</p><p class="text-sm text-muted-foreground leading-relaxed text-pretty">Kernel methods are a popular class of nonlinear predictive models in machine learning. Scalable algorithms for learning kernel models need to be iterative in nature,     but convergence can be slow due to poor conditioning. Spectral preconditioning is an important tool to speed-up the convergence of such iterative algorithms for training kernel models.     However computing and storing a spectral preconditioner can be expensive which can lead to large computational and storage overheads, precluding the application of kernel methods to problems with large datasets.     A Nystrom approximation of the spectral preconditioner is often cheaper to compute and store, and has demonstrated success in practical applications.     In this paper we analyze the trade-offs of using such an approximated preconditioner. Specifically, we show that a sample of logarithmic size (as a function of the size of the dataset) enables the Nyström-based approximated preconditioner     to accelerate gradient descent nearly as well as the exact preconditioner, while also reducing the computational and storage overheads.</p></div></div></div><div data-slot="card" class="bg-card text-card-foreground flex flex-col gap-6 rounded-xl border py-6 shadow-sm border-border hover:shadow-md transition-shadow"><div data-slot="card-content" class="p-4"><div class="space-y-3"><div class="flex items-start justify-between gap-4"><div class="flex-1"><h3 class="text-base font-semibold text-foreground text-balance leading-tight">On Feature Learning of Recursive Feature Machines and Automatic Relevance Determination</h3><p class="text-xs text-muted-foreground mt-1">D Gedon, A Abedsoltan, TB Schön, M Belkin</p></div><div class="flex flex-col items-end space-y-1"><div class="flex flex-wrap gap-1 justify-end"><span data-slot="badge" class="inline-flex items-center justify-center rounded-md border px-2 py-0.5 font-medium w-fit whitespace-nowrap shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden text-foreground [a&amp;]:hover:bg-accent [a&amp;]:hover:text-accent-foreground text-xs">Kernel methods</span><span data-slot="badge" class="inline-flex items-center justify-center rounded-md border px-2 py-0.5 font-medium w-fit whitespace-nowrap shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden text-foreground [a&amp;]:hover:bg-accent [a&amp;]:hover:text-accent-foreground text-xs">Feature Learning</span><span data-slot="badge" class="inline-flex items-center justify-center rounded-md border px-2 py-0.5 font-medium w-fit whitespace-nowrap shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden text-foreground [a&amp;]:hover:bg-accent [a&amp;]:hover:text-accent-foreground text-xs">Automatic relevance</span></div><span class="text-xs text-muted-foreground">2023</span></div></div><p class="text-xs font-medium text-primary">UniReps: the First Workshop on Unifying Representations in Neural Models</p><p class="text-sm text-muted-foreground leading-relaxed text-pretty">Feature learning is a crucial element for the performance of machine learning models. Recently, the exploration of feature learning in the context of kernel methods      has led to the introduction of Recursive Feature Machines (RFMs). In this work, we connect diagonal RFMs to Automatic Relevance Determination (ARD) from the Gaussian process literature.      We demonstrate that diagonal RFMs, similar to ARD, serve as a weighted covariate selection technique. However, they are trained using different paradigms: RFMs use recursive iterations      of the so-called Average Gradient Outer Product, while ARD employs maximum likelihood estimation. Our experiments show that while the learned features in both models correlate highly      across various tabular datasets, this correlation is lower for other datasets. Furthermore, we demonstrate that the RFM effectively captures correlation between covariates,      and we present instances where the RFM outperforms both ARD and diagonal RFM.</p></div></div></div><div data-slot="card" class="bg-card text-card-foreground flex flex-col gap-6 rounded-xl border py-6 shadow-sm border-border hover:shadow-md transition-shadow"><div data-slot="card-content" class="p-4"><div class="space-y-3"><div class="flex items-start justify-between gap-4"><div class="flex-1"><h3 class="text-base font-semibold text-foreground text-balance leading-tight">Toward Large Kernel Models</h3><p class="text-xs text-muted-foreground mt-1">A Abedsoltan, M Belkin, P Pandit</p></div><div class="flex flex-col items-end space-y-1"><div class="flex flex-wrap gap-1 justify-end"><span data-slot="badge" class="inline-flex items-center justify-center rounded-md border px-2 py-0.5 font-medium w-fit whitespace-nowrap shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden text-foreground [a&amp;]:hover:bg-accent [a&amp;]:hover:text-accent-foreground text-xs">Kernel methods</span><span data-slot="badge" class="inline-flex items-center justify-center rounded-md border px-2 py-0.5 font-medium w-fit whitespace-nowrap shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden text-foreground [a&amp;]:hover:bg-accent [a&amp;]:hover:text-accent-foreground text-xs">Nyström approximation</span><span data-slot="badge" class="inline-flex items-center justify-center rounded-md border px-2 py-0.5 font-medium w-fit whitespace-nowrap shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden text-foreground [a&amp;]:hover:bg-accent [a&amp;]:hover:text-accent-foreground text-xs">Preconditioning</span></div><span class="text-xs text-muted-foreground">2023</span></div></div><p class="text-xs font-medium text-primary">40th International Conference on Machine Learning (ICML2023)</p><p class="text-sm text-muted-foreground leading-relaxed text-pretty">Recent studies indicate that kernel machines can often perform similarly or better than deep neural networks (DNNs) on small datasets.     The interest in kernel machines has been additionally bolstered by the discovery of their equivalence to wide neural networks in certain regimes.     However, a key feature of DNNs is their ability to scale the model size and training data size independently, whereas in traditional kernel machines model     size is tied to data size. Because of this coupling, scaling kernel machines to large data has been computationally challenging. In this paper,     we provide a way forward for constructing large-scale general kernel models, which are a generalization of kernel machines that decouples the model and data,     allowing training on large datasets. Specifically, we introduce EigenPro 3.0, an algorithm based on projected dual preconditioned SGD and show scaling to model and data sizes     which have not been possible with existing kernel methods. We provide a PyTorch based implementation which can take advantage of multiple GPUs.</p></div></div></div><div data-slot="card" class="bg-card text-card-foreground flex flex-col gap-6 rounded-xl border py-6 shadow-sm border-border hover:shadow-md transition-shadow"><div data-slot="card-content" class="p-4"><div class="space-y-3"><div class="flex items-start justify-between gap-4"><div class="flex-1"><h3 class="text-base font-semibold text-foreground text-balance leading-tight">On emergence of clean-priority learning in early stopped neural networks</h3><p class="text-xs text-muted-foreground mt-1">C Liu, A Abedsoltan, M Belkin</p></div><div class="flex flex-col items-end space-y-1"><div class="flex flex-wrap gap-1 justify-end"><span data-slot="badge" class="inline-flex items-center justify-center rounded-md border px-2 py-0.5 font-medium w-fit whitespace-nowrap shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden text-foreground [a&amp;]:hover:bg-accent [a&amp;]:hover:text-accent-foreground text-xs">Neural networks</span><span data-slot="badge" class="inline-flex items-center justify-center rounded-md border px-2 py-0.5 font-medium w-fit whitespace-nowrap shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden text-foreground [a&amp;]:hover:bg-accent [a&amp;]:hover:text-accent-foreground text-xs">Early stopping</span><span data-slot="badge" class="inline-flex items-center justify-center rounded-md border px-2 py-0.5 font-medium w-fit whitespace-nowrap shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden text-foreground [a&amp;]:hover:bg-accent [a&amp;]:hover:text-accent-foreground text-xs">Optimization</span></div><span class="text-xs text-muted-foreground">2023</span></div></div><p class="text-xs font-medium text-primary">arXiv preprint arXiv:2306.02533</p><p class="text-sm text-muted-foreground leading-relaxed text-pretty">When random label noise is added to a training dataset, the prediction error of a neural network on a label-noise-free test dataset initially improves during early training but eventually deteriorates,      following a U-shaped dependence on training time. This behaviour is believed to be a result of neural networks learning the pattern of clean data first and fitting the noise later in the training,      a phenomenon that we refer to as clean-priority learning. In this study, we aim to explore the learning dynamics underlying this phenomenon. We theoretically demonstrate that, in the early stage of training,      the update direction of gradient descent is determined by the clean subset of training data, leaving the noisy subset has minimal to no impact, resulting in a prioritization of clean learning.      Moreover, we show both theoretically and experimentally, as the clean-priority learning goes on, the dominance of the gradients of clean samples over those of noisy samples diminishes,      and finally results in a termination of the clean-priority learning and fitting of the noisy samples.</p></div></div></div><div data-slot="card" class="bg-card text-card-foreground flex flex-col gap-6 rounded-xl border py-6 shadow-sm border-border hover:shadow-md transition-shadow"><div data-slot="card-content" class="p-4"><div class="space-y-3"><div class="flex items-start justify-between gap-4"><div class="flex-1"><h3 class="text-base font-semibold text-foreground text-balance leading-tight">Benign, tempered, or catastrophic: A taxonomy of overfitting</h3><p class="text-xs text-muted-foreground mt-1">N Mallinar, JB Simon, A Abedsoltan, P Pandit, M Belkin, P Nakkiran</p></div><div class="flex flex-col items-end space-y-1"><div class="flex flex-wrap gap-1 justify-end"><span data-slot="badge" class="inline-flex items-center justify-center rounded-md border px-2 py-0.5 font-medium w-fit whitespace-nowrap shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden text-foreground [a&amp;]:hover:bg-accent [a&amp;]:hover:text-accent-foreground text-xs">Overfitting</span><span data-slot="badge" class="inline-flex items-center justify-center rounded-md border px-2 py-0.5 font-medium w-fit whitespace-nowrap shrink-0 [&amp;&gt;svg]:size-3 gap-1 [&amp;&gt;svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden text-foreground [a&amp;]:hover:bg-accent [a&amp;]:hover:text-accent-foreground text-xs">Generalization</span></div><span class="text-xs text-muted-foreground">2022</span></div></div><p class="text-xs font-medium text-primary">36th Conference on Neural Information Processing Systems (NeurIPS 2022)</p><p class="text-sm text-muted-foreground leading-relaxed text-pretty">The practical success of overparameterized neural networks has motivated the recent scientific study of interpolating methods-- learning methods which are able fit their training data perfectly.     Empirically, certain interpolating methods can fit noisy training data without catastrophically bad test performance, which defies standard intuitions from statistical learning theory.      Aiming to explain this, a large body of recent work has studied benign overfitting, a behavior seen in certain asymptotic settings under which interpolating methods approach Bayes-optimality,       even in the presence of noise. In this work, we argue that, while benign overfitting has been instructive to study, real interpolating methods like deep networks do not fit benignly.        That is, noise in the train set leads to suboptimal generalization, suggesting that these methods fall in an intermediate regime between benign and catastrophic overfitting,         in which asymptotic risk is neither Bayes-optimal nor unbounded, with the confounding effect of the noise being tempered but non-negligible. We call this behavior tempered overfitting.         We first provide broad empirical evidence for our three-part taxonomy, demonstrating that deep neural networks and kernel machines fit to noisy data can be reasonably well classified as          benign, tempered, or catastrophic. We then specialize to kernel (ridge) regression (KR), obtaining conditions on the ridge parameter and kernel eigenspectrum under which KR exhibits each of the three behaviors,          demonstrating the consequences for KR with common kernels and trained neural networks of infinite width using experiments on natural and synthetic datasets.</p></div></div></div></div></section><div data-orientation="horizontal" role="none" data-slot="separator" class="bg-border shrink-0 data-[orientation=horizontal]:h-px data-[orientation=horizontal]:w-full data-[orientation=vertical]:h-full data-[orientation=vertical]:w-px my-8"></div><section id="hobbies" class="space-y-6"><div class="text-center space-y-2"><h2 class="text-2xl font-bold text-foreground text-balance">Personal Interests</h2><div class="w-16 h-0.5 bg-primary mx-auto rounded-full"></div></div><div class="grid md:grid-cols-3 gap-4"><div data-slot="card" class="bg-card text-card-foreground flex flex-col gap-6 rounded-xl border py-6 shadow-sm border-border hover:shadow-md transition-shadow"><div data-slot="card-content" class="p-4"><div class="flex items-start space-x-3"><div class="text-2xl flex-shrink-0">🎾</div><div class="space-y-1"><h3 class="text-base font-semibold text-foreground">Tennis</h3><p class="text-sm text-muted-foreground leading-relaxed text-pretty">I got into tennis because of Federer&#x27;s elegance, but somewhere along the way I ended up being blown away by Djokovic&#x27;s grit and brilliance!</p></div></div></div></div><div data-slot="card" class="bg-card text-card-foreground flex flex-col gap-6 rounded-xl border py-6 shadow-sm border-border hover:shadow-md transition-shadow"><div data-slot="card-content" class="p-4"><div class="flex items-start space-x-3"><div class="text-2xl flex-shrink-0">🥾</div><div class="space-y-1"><h3 class="text-base font-semibold text-foreground">Hiking</h3><p class="text-sm text-muted-foreground leading-relaxed text-pretty">Exploring mountain trails and nature reserves, combining physical activity with mindfulness.</p></div></div></div></div><div data-slot="card" class="bg-card text-card-foreground flex flex-col gap-6 rounded-xl border py-6 shadow-sm border-border hover:shadow-md transition-shadow"><div data-slot="card-content" class="p-4"><div class="flex items-start space-x-3"><div class="text-2xl flex-shrink-0">📚</div><div class="space-y-1"><h3 class="text-base font-semibold text-foreground"><a href="/books" class="hover:text-primary transition-colors">Reading</a></h3><p class="text-sm text-muted-foreground leading-relaxed text-pretty">I&#x27;m hooked on thought-provoking reads—check my books page for a peek at some of my latest and all-time favorites.</p></div></div></div></div></div></section></main><footer class="border-t border-border bg-card/50 mt-12"><div class="max-w-7xl mx-auto px-6 py-6"><div class="text-center space-y-3"><p class="text-sm text-muted-foreground">© 2025 Amirhesam Abedsoltan. All rights reserved.</p><div class="flex justify-center space-x-4"><a href="mailto:your.email@university.edu" class="text-sm text-muted-foreground hover:text-primary transition-colors">Email</a><a href="#" class="text-sm text-muted-foreground hover:text-primary transition-colors">LinkedIn</a><a href="#" class="text-sm text-muted-foreground hover:text-primary transition-colors">Google Scholar</a><a href="#" class="text-sm text-muted-foreground hover:text-primary transition-colors">ORCID</a></div></div></div></footer></div><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--><script src="/_next/static/chunks/webpack-62cf2d8cf4b76051.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/028c0d39d2e8f589-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/media/5b01f339abf2f1a5.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n3:HL[\"/_next/static/css/2a7316fc55aa7fb8.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"4:I[7266,[],\"\"]\n6:I[6849,[],\"ClientPageRoot\"]\n7:I[6585,[\"94\",\"static/chunks/94-81718c1dcd5eafa0.js\",\"931\",\"static/chunks/app/page-d10ce261fb8f3645.js\"],\"default\",1]\n8:I[2110,[],\"\"]\n9:I[4718,[],\"\"]\na:I[1781,[\"185\",\"static/chunks/app/layout-c96271b5eedd569a.js\"],\"Analytics\"]\nc:I[4735,[],\"\"]\nd:[]\n"])</script><script>self.__next_f.push([1,"0:[\"$\",\"$L4\",null,{\"buildId\":\"TJRZSFFbCIeOQRaO3keWd\",\"assetPrefix\":\"\",\"urlParts\":[\"\",\"\"],\"initialTree\":[\"\",{\"children\":[\"__PAGE__\",{}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"__PAGE__\",{},[[\"$L5\",[\"$\",\"$L6\",null,{\"props\":{\"params\":{},\"searchParams\":{}},\"Component\":\"$7\"}],null],null],null]},[[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/2a7316fc55aa7fb8.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"font-sans __variable_fb8f2c __variable_f910ec\",\"children\":[[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L9\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[]}],[\"$\",\"$La\",null,{}]]}]}]],null],null],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$Lb\"],\"globalErrorComponent\":\"$c\",\"missingSlots\":\"$Wd\"}]\n"])</script><script>self.__next_f.push([1,"b:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"v0 App\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Created with v0\"}],[\"$\",\"meta\",\"4\",{\"name\":\"generator\",\"content\":\"v0.app\"}],[\"$\",\"meta\",\"5\",{\"name\":\"next-size-adjust\"}]]\n5:null\n"])</script></body></html>