(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[931],{7452:function(e,t,a){Promise.resolve().then(a.bind(a,6585))},6585:function(e,t,a){"use strict";a.r(t),a.d(t,{default:function(){return A}});var r=a(288),n=a(5192),s=a(1676),i=a(119),o=a(7940),l=a(6056);let d=(0,o.j)("inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive",{variants:{variant:{default:"bg-primary text-primary-foreground shadow-xs hover:bg-primary/90",destructive:"bg-destructive text-white shadow-xs hover:bg-destructive/90 focus-visible:ring-destructive/20 dark:focus-visible:ring-destructive/40 dark:bg-destructive/60",outline:"border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50",secondary:"bg-secondary text-secondary-foreground shadow-xs hover:bg-secondary/80",ghost:"hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50",link:"text-primary underline-offset-4 hover:underline"},size:{default:"h-9 px-4 py-2 has-[>svg]:px-3",sm:"h-8 rounded-md gap-1.5 px-3 has-[>svg]:px-2.5",lg:"h-10 rounded-md px-6 has-[>svg]:px-4",icon:"size-9"}},defaultVariants:{variant:"default",size:"default"}});function c(e){let{className:t,variant:a,size:n,asChild:s=!1,...o}=e,c=s?i.g7:"button";return(0,r.jsx)(c,{"data-slot":"button",className:(0,l.cn)(d({variant:a,size:n,className:t})),...o})}var h=a(6835),m=a(4302);let u=e=>e.replace(/([a-z0-9])([A-Z])/g,"$1-$2").toLowerCase(),p=function(){for(var e=arguments.length,t=Array(e),a=0;a<e;a++)t[a]=arguments[a];return t.filter((e,t,a)=>!!e&&""!==e.trim()&&a.indexOf(e)===t).join(" ").trim()};var x={xmlns:"http://www.w3.org/2000/svg",width:24,height:24,viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:2,strokeLinecap:"round",strokeLinejoin:"round"};let g=(0,n.forwardRef)((e,t)=>{let{color:a="currentColor",size:r=24,strokeWidth:s=2,absoluteStrokeWidth:i,className:o="",children:l,iconNode:d,...c}=e;return(0,n.createElement)("svg",{ref:t,...x,width:r,height:r,stroke:a,strokeWidth:i?24*Number(s)/Number(r):s,className:p("lucide",o),...c},[...d.map(e=>{let[t,a]=e;return(0,n.createElement)(t,a)}),...Array.isArray(l)?l:[l]])}),f=(e,t)=>{let a=(0,n.forwardRef)((a,r)=>{let{className:s,...i}=a;return(0,n.createElement)(g,{ref:r,iconNode:t,className:p("lucide-".concat(u(e)),s),...i})});return a.displayName="".concat(e),a},b=f("Calendar",[["path",{d:"M8 2v4",key:"1cmpym"}],["path",{d:"M16 2v4",key:"4m81vk"}],["rect",{width:"18",height:"18",x:"3",y:"4",rx:"2",key:"1hopcy"}],["path",{d:"M3 10h18",key:"8toen8"}]]),v=f("Filter",[["polygon",{points:"22 3 2 3 10 12.46 10 19 14 21 14 12.46 22 3",key:"1yg77f"}]]),y=f("ArrowDownWideNarrow",[["path",{d:"m3 16 4 4 4-4",key:"1co6wj"}],["path",{d:"M7 20V4",key:"1yoxec"}],["path",{d:"M11 4h10",key:"1w87gc"}],["path",{d:"M11 8h7",key:"djye34"}],["path",{d:"M11 12h4",key:"q8tih4"}]]),w=f("ArrowUpNarrowWide",[["path",{d:"m3 8 4-4 4 4",key:"11wl7u"}],["path",{d:"M7 4v16",key:"1glfcx"}],["path",{d:"M11 12h4",key:"q8tih4"}],["path",{d:"M11 16h7",key:"uosisv"}],["path",{d:"M11 20h10",key:"jvxblo"}]]),k=[{id:2,title:"Task Generalization With AutoRegressive Compositional Structure: Can Learning From D Tasks Generalize to Dᵀ Tasks?",journal:"42nd International Conference on Machine Learning (ICML2025)",year:2025,topics:["In-context learning","Task generalization","Chain-Of-Thought"],date:"2025-08-01",authors:"A Abedsoltan, H Zhang, K Wan, H Lin, J Zhang, M Belkin",abstract:"Large language models (LLMs) exhibit remarkable task generalization, solving tasks they were never explicitly trained on with only a few demonstrations.     This raises a fundamental question: When can learning from a small set of tasks generalize to a large task family? In this paper, we investigate task generalization     through the lens of autoregressive compositional structure, where each task is a composition of T operations, and each operation is among a finite family of d subtasks.      This yields a total class of size d^T. We first show that generalization to all d^T tasks is theoretically achievable by training on only \xd5(d) tasks.       Empirically, we demonstrate that Transformers achieve such exponential task generalization on sparse parity functions via In-context Learning (ICL) and chain-of-thought (CoT) reasoning.        We further show generalization in arithmetic and translation, beyond parity functions."},{id:1,title:"Fast training of large kernel models with delayed projections",journal:"39th Conference on Neural Information Processing Systems (NeurIPS 2025-Spotlight)",year:2025,topics:["Kernel methods","Optimization","Preconditioning"],date:"2024-11-25",authors:"A Abedsoltan, S Ma, P Pandit, M Belkin",abstract:"Classical kernel machines have historically faced significant challenges in scaling to large datasets and model sizes--a key ingredient that has driven the success of neural networks.      In this paper, we present a new methodology for building kernel machines that can scale efficiently with both data size and model size.      Our algorithm introduces delayed projections to Preconditioned Stochastic Gradient Descent (PSGD) allowing the training of much larger models than was previously feasible,       pushing the practical limits of kernel-based learning. We validate our algorithm, EigenPro4, across multiple datasets, demonstrating drastic training speed up over       the existing methods while maintaining comparable or better classification accuracy."},{id:3,title:"Context-Scaling versus Task-Scaling in In-Context Learning",journal:"arXiv preprint arXiv:2410.12783",year:2024,topics:["In-context learning","Transformers"],date:"2024-10-16",authors:"A Abedsoltan, A Radhakrishnan, J Wu, M Belkin",abstract:"Transformers exhibit In-Context Learning (ICL), where these models solve new tasks by using examples in the prompt without additional training.     In our work, we identify and analyze two key components of ICL: (1) context-scaling, where model performance improves as the number of in-context examples increases and      (2) task-scaling, where model performance improves as the number of pre-training tasks increases. While transformers are capable of both context-scaling and task-scaling,       we empirically show that standard Multi-Layer Perceptrons (MLPs) with vectorized input are only capable of task-scaling. To understand how transformers are capable of context-scaling,        we first propose a significantly simplified transformer architecture without key, query, value weights. We show that it performs ICL comparably to the original GPT-2 model        in various statistical learning tasks including linear regression, teacher-student settings. Furthermore, a single block of our simplified transformer can be viewed as data dependent         feature map followed by an MLP. This feature map on its own is a powerful predictor that is capable of context-scaling but is not capable of task-scaling. We show empirically that concatenating          the output of this feature map with vectorized data as an input to MLPs enables both context-scaling and task-scaling. This finding provides a simple setting to study context and task-scaling for ICL."},{id:4,title:"On the Nystr\xf6m approximation for preconditioning in kernel machines",journal:"International Conference on Artificial Intelligence and Statistics (AISTATS 2024)",year:2024,topics:["Kernel methods","Nystr\xf6m approximation","Preconditioning"],date:"2024-05-02",authors:"A Abedsoltan, P Pandit, L Rademacher, M Belkin",abstract:"Add your abstract here later."},{id:5,title:"Uncertainty estimation with recursive feature machines",journal:"The 40th Conference on Uncertainty in Artificial Intelligence",year:2024,topics:["Uncertainty estimation","Feature Learning","Kernel methods"],date:"2024-07-15",authors:"D Gedon, A Abedsoltan, TB Sch\xf6n, M Belkin",abstract:"In conventional regression analysis, predictions are typically represented as point estimates derived from covariates.      The Gaussian Process (GP) offer a kernel-based framework that predicts and additionally quantifies associated uncertainties.       However, kernel-based methods often underperform ensemble-based decision tree approaches in regression tasks involving tabular and categorical data.        Recently, Recursive Feature Machines (RFMs) were proposed as a novel feature-learning kernel which strengthens the capabilities of kernel machines.        In this study, we harness the power RFMs in a probabilistic GP-based approach to enhance uncertainty estimation through feature extraction within kernel methods.         We employ this learned kernel for in-depth uncertainty analysis. On tabular datasets, our RFM-based method surpasses other leading uncertainty estimation techniques,         including NGBoost and CatBoost-ensemble. Additionally, when assessing out-of-distribution performance, we found that boosting-based methods are surpassed by our RFM-based approach."},{id:6,title:"On emergence of clean-priority learning in early stopped neural networks",journal:"arXiv preprint arXiv:2306.02533",year:2023,topics:["Neural networks","Early stopping","Optimization"],date:"2023-06-05",authors:"C Liu, A Abedsoltan, M Belkin",abstract:"Add your abstract here later."},{id:7,title:"Toward Large Kernel Models",journal:"40th International Conference on Machine Learning (ICML2023)",year:2023,topics:["Kernel methods","Nystr\xf6m approximation","Preconditioning"],date:"2023-07-23",authors:"A Abedsoltan, M Belkin, P Pandit",abstract:"Add your abstract here later."},{id:8,title:"Benign, tempered, or catastrophic: A taxonomy of overfitting",journal:"36th Conference on Neural Information Processing Systems (NeurIPS 2022)",year:2022,topics:["Overfitting","Generalization"],date:"2022-12-06",authors:"N Mallinar, JB Simon, A Abedsoltan, P Pandit, M Belkin, P Nakkiran",abstract:"The practical success of overparameterized neural networks has motivated the recent scientific study of interpolating methods-- learning methods which are able fit their training data perfectly.     Empirically, certain interpolating methods can fit noisy training data without catastrophically bad test performance, which defies standard intuitions from statistical learning theory.      Aiming to explain this, a large body of recent work has studied benign overfitting, a behavior seen in certain asymptotic settings under which interpolating methods approach Bayes-optimality,       even in the presence of noise. In this work, we argue that, while benign overfitting has been instructive to study, real interpolating methods like deep networks do not fit benignly.        That is, noise in the train set leads to suboptimal generalization, suggesting that these methods fall in an intermediate regime between benign and catastrophic overfitting,         in which asymptotic risk is neither Bayes-optimal nor unbounded, with the confounding effect of the noise being tempered but non-negligible. We call this behavior tempered overfitting.         We first provide broad empirical evidence for our three-part taxonomy, demonstrating that deep neural networks and kernel machines fit to noisy data can be reasonably well classified as          benign, tempered, or catastrophic. We then specialize to kernel (ridge) regression (KR), obtaining conditions on the ridge parameter and kernel eigenspectrum under which KR exhibits each of the three behaviors,          demonstrating the consequences for KR with common kernels and trained neural networks of infinite width using experiments on natural and synthetic datasets."},{id:9,title:"On Feature Learning of Recursive Feature Machines and Automatic Relevance Determination",journal:"UniReps: the First Workshop on Unifying Representations in Neural Models",year:2023,topics:["Feature learning","Feature Learning","Automatic relevance"],date:"2023-09-18",authors:"D Gedon, A Abedsoltan, TB Sch\xf6n, M Belkin",abstract:"Feature learning is a crucial element for the performance of machine learning models. Recently, the exploration of feature learning in the context of kernel methods      has led to the introduction of Recursive Feature Machines (RFMs). In this work, we connect diagonal RFMs to Automatic Relevance Determination (ARD) from the Gaussian process literature.      We demonstrate that diagonal RFMs, similar to ARD, serve as a weighted covariate selection technique. However, they are trained using different paradigms: RFMs use recursive iterations      of the so-called Average Gradient Outer Product, while ARD employs maximum likelihood estimation. Our experiments show that while the learned features in both models correlate highly      across various tabular datasets, this correlation is lower for other datasets. Furthermore, we demonstrate that the RFM effectively captures correlation between covariates,      and we present instances where the RFM outperforms both ARD and diagonal RFM."}],j=[{id:1,title:"Our paper, “Fast Training of Large Kernel Models with Delayed Projections.” was spotlighted at NeurIPS.",date:"September 2025",description:"In this paper, we significantly reduced the runtime of kernel solvers from days to minutes by using delayed projections in Preconditioned Stochastic Gradient Descent."},{id:2,title:"I started an AI resaerch intern at Figma.",date:"June 2025",description:"I will be working on multi-agent workflows for prompt-to-design applications extending agent-squad from AWS."},{id:3,title:"Our paper, “Task Generalization With AutoRegressive Compositional Structure: Can Learning From D Tasks Generalize to Dᵀ Tasks?” was accepted at ICML.",date:"August 2025",description:"We study task generalization through an autoregressive compositional framework, showing that training on only a small subset of tasks enables exponential generalization to a much larger class.     As an example, we demonstrate that Transformers trained on sparse parity tasks generalize in-context through chain-of-thought reasoning, and further extend to arithmetic and translation."}],N=[{name:"Tennis",description:"I got into tennis because of Federer's elegance, but somewhere along the way I ended up being blown away by Djokovic's grit and brilliance!",icon:"\uD83C\uDFBE"},{name:"Hiking",description:"Exploring mountain trails and nature reserves, combining physical activity with mindfulness.",icon:"\uD83E\uDD7E"},{name:"Reading",description:"I'm hooked on thought-provoking reads—check my books page for a peek at some of my latest and all-time favorites.",icon:"\uD83D\uDCDA",link:"/books"}];function A(){let[e,t]=(0,n.useState)("date"),[a,i]=(0,n.useState)("desc"),[o,l]=(0,n.useState)("all"),d=Array.from(new Set(k.flatMap(e=>e.topics))),u=k.filter(e=>"all"===o||e.topics.includes(o)).sort((t,r)=>{if("date"!==e)return"desc"===a?r.topics[0].localeCompare(t.topics[0]):t.topics[0].localeCompare(r.topics[0]);{let e=new Date(t.date).getTime(),n=new Date(r.date).getTime();return"desc"===a?n-e:e-n}});return(0,r.jsxs)("div",{className:"min-h-screen bg-background",children:[(0,r.jsx)("header",{className:"border-b border-border bg-card/50 backdrop-blur-sm sticky top-0 z-10",children:(0,r.jsx)("div",{className:"max-w-7xl mx-auto px-6 py-4",children:(0,r.jsxs)("div",{className:"flex items-center justify-between",children:[(0,r.jsxs)("div",{children:[(0,r.jsx)("h1",{className:"text-2xl font-bold text-foreground text-balance",children:"Amirhesam Abedsoltan"}),(0,r.jsx)("p",{className:"text-sm text-muted-foreground",children:"PhD Candidate at UC San Diego"})]}),(0,r.jsxs)("nav",{className:"hidden md:flex space-x-4",children:[(0,r.jsx)("a",{href:"#bio",className:"text-sm text-muted-foreground hover:text-primary transition-colors",children:"About"}),(0,r.jsx)("a",{href:"#news",className:"text-sm text-muted-foreground hover:text-primary transition-colors",children:"News"}),(0,r.jsx)("a",{href:"#publications",className:"text-sm text-muted-foreground hover:text-primary transition-colors",children:"Publications"}),(0,r.jsx)("a",{href:"#hobbies",className:"text-sm text-muted-foreground hover:text-primary transition-colors",children:"Hobbies"})]})]})})}),(0,r.jsxs)("main",{className:"max-w-7xl mx-auto px-6 py-8 space-y-10",children:[(0,r.jsxs)("section",{id:"bio",className:"space-y-6",children:[(0,r.jsxs)("div",{className:"text-center space-y-2",children:[(0,r.jsx)("h2",{className:"text-2xl font-bold text-foreground text-balance",children:"About Me"}),(0,r.jsx)("div",{className:"w-16 h-0.5 bg-primary mx-auto rounded-full"})]}),(0,r.jsxs)("div",{className:"grid md:grid-cols-4 gap-8 items-start",children:[(0,r.jsx)("div",{className:"flex justify-center",children:(0,r.jsx)("div",{className:"w-48 h-48 bg-muted rounded-full flex items-center justify-center overflow-hidden shadow-lg",children:(0,r.jsx)("img",{src:"/profile.jpg",alt:"Profile photo",className:"w-full h-full object-cover"})})}),(0,r.jsxs)("div",{className:"md:col-span-3 space-y-4",children:[(0,r.jsx)("div",{className:"prose max-w-none",children:(0,r.jsxs)("p",{className:"text-base text-foreground leading-relaxed text-pretty",children:["Hi, I'm Amirhesam Abedsoltan, a Ph.D. candidate at UC San Diego. I was lucky to be advised by"," ",(0,r.jsx)("a",{href:"https://misha.belkin-wang.org/",target:"_blank",rel:"noopener noreferrer",className:"text-primary hover:underline",children:"Prof. Mikhail Belkin"}),". My work broadly focuses on machine learning, spanning both theory and practice, with an interest in scaling methods and understanding how modern AI systems learn and generalize."]})}),(0,r.jsxs)("div",{className:"space-y-3",children:[(0,r.jsx)("h3",{className:"text-lg font-semibold text-foreground",children:"Education"}),(0,r.jsxs)("div",{className:"space-y-2 text-sm",children:[(0,r.jsxs)("div",{className:"flex items-center space-x-2",children:[(0,r.jsx)("div",{className:"w-1.5 h-1.5 bg-primary rounded-full flex-shrink-0"}),(0,r.jsxs)("div",{children:[(0,r.jsx)("span",{className:"font-medium text-foreground",children:"Ph.D. Computer Science"}),(0,r.jsx)("span",{className:"text-muted-foreground ml-2",children:"University of California San Diego, 2021 - Present"})]})]}),(0,r.jsxs)("div",{className:"flex items-center space-x-2",children:[(0,r.jsx)("div",{className:"w-1.5 h-1.5 bg-primary rounded-full flex-shrink-0"}),(0,r.jsxs)("div",{children:[(0,r.jsx)("span",{className:"font-medium text-foreground",children:"M.S. Computer Science"}),(0,r.jsx)("span",{className:"text-muted-foreground ml-2",children:"University of Southern California, 2021"})]})]}),(0,r.jsxs)("div",{className:"flex items-center space-x-2",children:[(0,r.jsx)("div",{className:"w-1.5 h-1.5 bg-primary rounded-full flex-shrink-0"}),(0,r.jsxs)("div",{children:[(0,r.jsx)("span",{className:"font-medium text-foreground",children:"B.S. Electrical Engineering"}),(0,r.jsx)("span",{className:"text-muted-foreground ml-2",children:"Sharif University of Technology, 2014"})]})]})]})]})]})]})]}),(0,r.jsx)(m.Z,{className:"my-8"}),(0,r.jsxs)("section",{id:"news",className:"space-y-6",children:[(0,r.jsxs)("div",{className:"text-center space-y-2",children:[(0,r.jsx)("h2",{className:"text-2xl font-bold text-foreground text-balance",children:"Recent News"}),(0,r.jsx)("div",{className:"w-16 h-0.5 bg-primary mx-auto rounded-full"})]}),(0,r.jsx)("div",{className:"space-y-3",children:j.map(e=>(0,r.jsx)(s.Zb,{className:"border-border hover:shadow-md transition-shadow",children:(0,r.jsx)(s.aY,{className:"p-4",children:(0,r.jsxs)("div",{className:"flex items-start space-x-3",children:[(0,r.jsx)(b,{className:"w-4 h-4 text-primary mt-0.5 flex-shrink-0"}),(0,r.jsxs)("div",{className:"flex-1 space-y-1",children:[(0,r.jsxs)("div",{className:"flex items-start justify-between gap-4",children:[(0,r.jsx)("h3",{className:"text-base font-semibold text-foreground text-balance leading-tight",children:e.title}),(0,r.jsx)(h.C,{variant:"secondary",className:"text-xs whitespace-nowrap",children:e.date})]}),(0,r.jsx)("p",{className:"text-sm text-muted-foreground leading-relaxed text-pretty",children:e.description})]})]})})},e.id))})]}),(0,r.jsx)(m.Z,{className:"my-8"}),(0,r.jsxs)("section",{id:"publications",className:"space-y-6",children:[(0,r.jsxs)("div",{className:"text-center space-y-2",children:[(0,r.jsx)("h2",{className:"text-2xl font-bold text-foreground text-balance",children:"Selected Publications"}),(0,r.jsx)("div",{className:"w-16 h-0.5 bg-primary mx-auto rounded-full"})]}),(0,r.jsxs)("div",{className:"flex flex-wrap gap-3 items-center justify-between bg-card p-3 rounded-lg border border-border",children:[(0,r.jsx)("div",{className:"flex items-center space-x-3",children:(0,r.jsxs)("div",{className:"flex items-center space-x-2",children:[(0,r.jsx)(v,{className:"w-3 h-3 text-muted-foreground"}),(0,r.jsxs)("select",{value:o,onChange:e=>l(e.target.value),className:"bg-background border border-border rounded px-2 py-1 text-xs",children:[(0,r.jsx)("option",{value:"all",children:"All Topics"}),d.map(e=>(0,r.jsx)("option",{value:e,children:e},e))]})]})}),(0,r.jsxs)("div",{className:"flex items-center space-x-2",children:[(0,r.jsx)("span",{className:"text-xs text-muted-foreground",children:"Sort:"}),(0,r.jsx)(c,{variant:"date"===e?"default":"outline",size:"sm",onClick:()=>t("date"),className:"text-xs h-7",children:"Date"}),(0,r.jsx)(c,{variant:"topic"===e?"default":"outline",size:"sm",onClick:()=>t("topic"),className:"text-xs h-7",children:"Topic"}),(0,r.jsx)(c,{variant:"outline",size:"sm",onClick:()=>i("desc"===a?"asc":"desc"),className:"text-xs h-7 px-2",children:"desc"===a?(0,r.jsx)(y,{className:"w-3 h-3"}):(0,r.jsx)(w,{className:"w-3 h-3"})})]})]}),(0,r.jsx)("div",{className:"space-y-4",children:u.map(e=>(0,r.jsx)(s.Zb,{className:"border-border hover:shadow-md transition-shadow",children:(0,r.jsx)(s.aY,{className:"p-4",children:(0,r.jsxs)("div",{className:"space-y-3",children:[(0,r.jsxs)("div",{className:"flex items-start justify-between gap-4",children:[(0,r.jsxs)("div",{className:"flex-1",children:[(0,r.jsx)("h3",{className:"text-base font-semibold text-foreground text-balance leading-tight",children:e.title}),(0,r.jsx)("p",{className:"text-xs text-muted-foreground mt-1",children:e.authors})]}),(0,r.jsxs)("div",{className:"flex flex-col items-end space-y-1",children:[(0,r.jsx)("div",{className:"flex flex-wrap gap-1 justify-end",children:e.topics.map((e,t)=>(0,r.jsx)(h.C,{variant:"outline",className:"text-xs",children:e},t))}),(0,r.jsx)("span",{className:"text-xs text-muted-foreground",children:e.year})]})]}),(0,r.jsx)("p",{className:"text-xs font-medium text-primary",children:e.journal}),(0,r.jsx)("p",{className:"text-sm text-muted-foreground leading-relaxed text-pretty",children:e.abstract})]})})},e.id))})]}),(0,r.jsx)(m.Z,{className:"my-8"}),(0,r.jsxs)("section",{id:"hobbies",className:"space-y-6",children:[(0,r.jsxs)("div",{className:"text-center space-y-2",children:[(0,r.jsx)("h2",{className:"text-2xl font-bold text-foreground text-balance",children:"Personal Interests"}),(0,r.jsx)("div",{className:"w-16 h-0.5 bg-primary mx-auto rounded-full"})]}),(0,r.jsx)("div",{className:"grid md:grid-cols-2 gap-4",children:N.map((e,t)=>(0,r.jsx)(s.Zb,{className:"border-border hover:shadow-md transition-shadow",children:(0,r.jsx)(s.aY,{className:"p-4",children:(0,r.jsxs)("div",{className:"flex items-start space-x-3",children:[(0,r.jsx)("div",{className:"text-2xl flex-shrink-0",children:e.icon}),(0,r.jsxs)("div",{className:"space-y-1",children:[(0,r.jsx)("h3",{className:"text-base font-semibold text-foreground",children:e.link?(0,r.jsx)("a",{href:e.link,className:"hover:text-primary transition-colors",children:e.name}):e.name}),(0,r.jsx)("p",{className:"text-sm text-muted-foreground leading-relaxed text-pretty",children:e.description})]})]})})},t))})]})]}),(0,r.jsx)("footer",{className:"border-t border-border bg-card/50 mt-12",children:(0,r.jsx)("div",{className:"max-w-7xl mx-auto px-6 py-6",children:(0,r.jsxs)("div",{className:"text-center space-y-3",children:[(0,r.jsx)("p",{className:"text-sm text-muted-foreground",children:"\xa9 2025 Amirhesam Abedsoltan. All rights reserved."}),(0,r.jsxs)("div",{className:"flex justify-center space-x-4",children:[(0,r.jsx)("a",{href:"mailto:your.email@university.edu",className:"text-sm text-muted-foreground hover:text-primary transition-colors",children:"Email"}),(0,r.jsx)("a",{href:"#",className:"text-sm text-muted-foreground hover:text-primary transition-colors",children:"LinkedIn"}),(0,r.jsx)("a",{href:"#",className:"text-sm text-muted-foreground hover:text-primary transition-colors",children:"Google Scholar"}),(0,r.jsx)("a",{href:"#",className:"text-sm text-muted-foreground hover:text-primary transition-colors",children:"ORCID"})]})]})})})]})}},6835:function(e,t,a){"use strict";a.d(t,{C:function(){return l}});var r=a(288);a(5192);var n=a(119),s=a(7940),i=a(6056);let o=(0,s.j)("inline-flex items-center justify-center rounded-md border px-2 py-0.5 text-xs font-medium w-fit whitespace-nowrap shrink-0 [&>svg]:size-3 gap-1 [&>svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden",{variants:{variant:{default:"border-transparent bg-primary text-primary-foreground [a&]:hover:bg-primary/90",secondary:"border-transparent bg-secondary text-secondary-foreground [a&]:hover:bg-secondary/90",destructive:"border-transparent bg-destructive text-white [a&]:hover:bg-destructive/90 focus-visible:ring-destructive/20 dark:focus-visible:ring-destructive/40 dark:bg-destructive/60",outline:"text-foreground [a&]:hover:bg-accent [a&]:hover:text-accent-foreground"}},defaultVariants:{variant:"default"}});function l(e){let{className:t,variant:a,asChild:s=!1,...l}=e,d=s?n.g7:"span";return(0,r.jsx)(d,{"data-slot":"badge",className:(0,i.cn)(o({variant:a}),t),...l})}},1676:function(e,t,a){"use strict";a.d(t,{Zb:function(){return s},aY:function(){return i}});var r=a(288);a(5192);var n=a(6056);function s(e){let{className:t,...a}=e;return(0,r.jsx)("div",{"data-slot":"card",className:(0,n.cn)("bg-card text-card-foreground flex flex-col gap-6 rounded-xl border py-6 shadow-sm",t),...a})}function i(e){let{className:t,...a}=e;return(0,r.jsx)("div",{"data-slot":"card-content",className:(0,n.cn)("px-6",t),...a})}},4302:function(e,t,a){"use strict";a.d(t,{Z:function(){return i}});var r=a(288);a(5192);var n=a(8218),s=a(6056);function i(e){let{className:t,orientation:a="horizontal",decorative:i=!0,...o}=e;return(0,r.jsx)(n.f,{"data-slot":"separator",decorative:i,orientation:a,className:(0,s.cn)("bg-border shrink-0 data-[orientation=horizontal]:h-px data-[orientation=horizontal]:w-full data-[orientation=vertical]:h-full data-[orientation=vertical]:w-px",t),...o})}},6056:function(e,t,a){"use strict";a.d(t,{cn:function(){return s}});var r=a(9773),n=a(7009);function s(){for(var e=arguments.length,t=Array(e),a=0;a<e;a++)t[a]=arguments[a];return(0,n.m6)((0,r.W)(t))}}},function(e){e.O(0,[94,24,240,744],function(){return e(e.s=7452)}),_N_E=e.O()}]);